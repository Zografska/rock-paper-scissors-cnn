{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a4ad04",
   "metadata": {},
   "source": [
    "Rock - Paper - Scissors CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7719119",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 17\n",
    "image_size = (150, 150)\n",
    "batch_size = 710\n",
    "classes = (\"paper\", \"rock\", \"scissors\")\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c183c",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "Explore the dataset thoroughly and provide a summary of your observations.\n",
    "Perform necessary preprocessing steps:\n",
    "    - [x] Explore and plot the data\n",
    "    - [x] Image resizing.\n",
    "    - [x] Image normalization.\n",
    "    - [x] Optionally, data augmentation techniques.\n",
    "    - [x] Splitting the data into training and test sets appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"data_reconstructed/train\",\n",
    "    image_size=image_size,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    \n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"data_reconstructed/validation\",\n",
    "    image_size=image_size,\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"data_reconstructed/test\",\n",
    "    image_size=image_size,\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d74a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(train_ds, classes):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in train_ds.take(1):\n",
    "        for i in range(12):\n",
    "            plt.subplot(4, 3, i + 1)\n",
    "            plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "            plt.title(classes[int(labels[i])])\n",
    "            plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(initial_train_ds, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\", seed=seed),\n",
    "    layers.RandomBrightness(0.2, seed=seed),\n",
    "    layers.RandomZoom(0.2, seed=seed),\n",
    "    layers.RandomContrast(0.4, seed=seed),\n",
    "    layers.RandomCrop(height=image_size[0], width=image_size[1], seed=seed),\n",
    "])\n",
    "\n",
    "augmented_train_ds = initial_train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(augmented_train_ds, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d52f7",
   "metadata": {},
   "source": [
    "### Image normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47db72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_ds = augmented_train_ds.map(normalize)\n",
    "val_ds = val_ds.map(normalize)\n",
    "test_ds = test_ds.map(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca859be",
   "metadata": {},
   "source": [
    "### Convert dataset to numpy array and split into feature matrix and label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7831c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataset_to_numpy(ds):\n",
    "    X, y = [], []\n",
    "    for images, labels in ds:\n",
    "        X.append(images.numpy())\n",
    "        y.append(labels.numpy())\n",
    "    return tf.concat(X, axis=0), tf.concat(y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataset_to_numpy(train_ds)\n",
    "X_validation, y_validation = dataset_to_numpy(val_ds)\n",
    "X_test, y_test = dataset_to_numpy(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e406638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.unique(y_train,return_counts=True),np.unique(y_validation,return_counts=True), np.unique(y_test,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_train_np = y_train.numpy() if hasattr(y_train, 'numpy') else np.array(y_train)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_np),\n",
    "    y=y_train_np\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(class_weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54300e8a",
   "metadata": {},
   "source": [
    "The dataset is not imbalanced since the differences between the occurance of classes is not big."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4736670",
   "metadata": {},
   "source": [
    "Simple model\n",
    "- one convolutional layer, 32 fiters 3x3 grid\n",
    "- maxpooling 2x2\n",
    "- hidden layer NN with 512 neurons\n",
    "- softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(image_size[0], image_size[1], 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Int('conv_filters', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "    ))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense( \n",
    "        units = hp.Int('dense_units', min_value=128, max_value=512, step=128), \n",
    "        activation='relu'))\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4], default=1e-3)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f8e52",
   "metadata": {},
   "source": [
    "### Tuning the model\n",
    "Tuning params\n",
    " - Number of filters for the Convolutional Layer\n",
    " - Number of units for the densely-connected NN layer\n",
    " - Learning rate for the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b063985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_simple_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=30,\n",
    "    directory='tuning',\n",
    "    project_name='cnn_tuning'\n",
    ")\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34595672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuner.search(train_ds, validation_data=val_ds, epochs=30, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_1 = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparams = tuner.get_best_hyperparameters(1)[0]\n",
    "best_learning_rate = best_hyperparams.get('learning_rate')\n",
    "best_conv_filters = best_hyperparams.get('conv_filters')\n",
    "best_dense_units = best_hyperparams.get('dense_units')\n",
    "best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_epoch = best_trial.hyperparameters.get('tuner/epochs')\n",
    "\n",
    "print(\"Best Conv filters:\", best_hyperparams.get('conv_filters'))\n",
    "print(\"Best Dense units:\", best_hyperparams.get('dense_units'))\n",
    "print(\"Best Learning rate:\", best_hyperparams.get('learning_rate'))\n",
    "print(\"Best Epoch:\", best_epoch)\n",
    "\n",
    "best_model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec00d2e",
   "metadata": {},
   "source": [
    "### Manually tuning the batch size on the best parameters model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "batch_sizes = [16, 32, 64]  \n",
    "results = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Training with batch size: {batch_size}\")\n",
    "    \n",
    "    model = best_model_1\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=batch_size, validation_data=(X_validation, y_validation), callbacks=[stop_early])\n",
    "\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    results.append((batch_size, val_accuracy))\n",
    "    print(f\"Validation Accuracy for batch size {batch_size}: {val_accuracy}\")\n",
    "\n",
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "best_batch_size, best_val_accuracy = results[0]\n",
    "print(f\"Best Batch Size: {best_batch_size}, Best Validation Accuracy: {best_val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba36a4",
   "metadata": {},
   "source": [
    "### Used tuned params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b184ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_1.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=best_learning_rate),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "best_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "simple_history = best_model_1.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_validation, y_validation),\n",
    "    epochs=5,\n",
    "    callbacks=[early_stopping, checkpointer],\n",
    "    batch_size=32\n",
    ").history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11cd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614552a3",
   "metadata": {},
   "source": [
    "### Simple Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca537a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(simple_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a392c48",
   "metadata": {},
   "source": [
    "### Provide visualizations of training curves (loss and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_improvement = Sequential()\n",
    "model_1_improvement.add(Input(shape=(image_size[0], image_size[1], 3)))\n",
    "model_1_improvement.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model_1_improvement.add(MaxPooling2D(2,2))\n",
    "model_1_improvement.add(Dropout(0.15))\n",
    "model_1_improvement.add(Flatten())\n",
    "model_1_improvement.add(Dense(512, activation='relu'))\n",
    "model_1_improvement.add(Dropout(0.3))\n",
    "model_1_improvement.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model_1_improvement.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=best_learning_rate),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model_1_improvement.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history_improvement = model_1_improvement.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_validation, y_validation),\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping],\n",
    "    batch_size=32, \n",
    "    class_weight=class_weights_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ace114d",
   "metadata": {},
   "source": [
    "### Improvement Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cee6440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred = np.argmax(y_pred, axis=1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d8da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77fdf34",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "#### Simple Best Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca345b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(best_model_1, X_test)\n",
    "plot_confusion_matrix(y_test, pred, classes)\n",
    "print(classification_report(y_test,pred,target_names = classes, digits=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513d600",
   "metadata": {},
   "source": [
    "#### Improved Simple Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b0fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_improvement = predict(model_1_improvement, X_test)\n",
    "print(classification_report(y_test,pred_improvement,target_names = classes, digits=7))\n",
    "plot_confusion_matrix(y_test, pred_improvement, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a6c10",
   "metadata": {},
   "source": [
    "### Making a more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Input(shape=(150, 150, 3)))\n",
    "\n",
    "model_2.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model_2.add(MaxPooling2D(2, 2))\n",
    "model_2.add(Dropout(0.1))\n",
    "\n",
    "model_2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_2.add(MaxPooling2D(2, 2))\n",
    "model_2.add(Dropout(0.1))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(units=128, activation='relu'))\n",
    "model_2.add(Dropout(0.1))\n",
    "model_2.add(Dense(units=512, activation='relu'))\n",
    "model_2.add(Dropout(0.1))\n",
    "model_2.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model_2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                               factor=0.1, \n",
    "                               patience=2, \n",
    "                               min_lr=0.000001)\n",
    "history_2 = model_2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_validation, y_validation),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[reduce_lr, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b45126",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c7ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_2.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_classes, classes)\n",
    "print(classification_report(y_test, y_pred_classes, digits=7, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c2229",
   "metadata": {},
   "source": [
    "### Check generalization on the More Complex Model on Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2150327",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalization_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"generalization_bg_removed\",\n",
    "    image_size=image_size,\n",
    "    shuffle=False,\n",
    "    seed=12,\n",
    ")\n",
    "\n",
    "# rotate the images because the pictures are 300x200 instead of 200x300\n",
    "def rotate_image(image, label):\n",
    "    image = tf.image.rot90(image, k=1)\n",
    "    return image, label\n",
    "generalization_ds = generalization_ds.map(rotate_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af39daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in generalization_ds.take(1):\n",
    "    for i in range(3):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "        \n",
    "\n",
    "normalized_ds = generalization_ds.map(normalize)\n",
    "X_generalization, y_generalization = dataset_to_numpy(normalized_ds)\n",
    "\n",
    "y_pred = model_2.predict(X_generalization)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "plot_confusion_matrix(y_generalization, y_pred_classes, classes)\n",
    "print(classification_report(y_generalization, y_pred_classes, digits=7, target_names=classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
